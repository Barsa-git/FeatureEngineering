{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "bGIhXUPoaq7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment Questions : -"
      ],
      "metadata": {
        "id": "Q22NeLWNav7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is a parameter?\n",
        "- In the context of feature engineering, a parameter refers to a characteristic or variable that defines the behavior or characteristics of a feature, often used to represent the underlying relationships within the data. Essentially, parameters are attributes of a feature that are used by the model to make predictions.\n",
        "- Here's a more detailed explanation :\n",
        "  - Features  :\n",
        "    - These are the input variables used by a machine learning model to make predictions. They are the characteristics or attributes of the data that the model learns from.\n",
        "  - Parameters :\n",
        "    - These are the specific values or characteristics that define a feature. They are used to represent how a feature contributes to the overall model's output. For example, in a linear regression model, the parameters might be the coefficients that determine the strength and direction of the relationship between a feature and the target variable.\n",
        "  - Example :\n",
        "    - If a feature is \"house size\" and the model uses a linear relationship to estimate house price, the parameters might be the slope and intercept of the linear line, which define the relationship between house size and price."
      ],
      "metadata": {
        "id": "5pyK4TLLa5-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is correlation?What does negative correlation mean?\n",
        "- In machine learning, correlation refers to the statistical relationship between two or more variables, quantifying how much they tend to change together.\n",
        "- It's a measure of association, not causation, meaning a strong correlation doesn't imply one variable causes the other. Correlation coefficients, like the Pearson correlation, range from -1 to +1, with values closer to 1 or -1 indicating stronger relationships and values closer to 0 indicating weaker or no linear relationship.\n",
        "- In Machine Learning, a negative correlation between two variables means that when one variable increases, the other tends to decrease, and vice versa. This relationship is often visualized as a downward-sloping line on a scatter plot."
      ],
      "metadata": {
        "id": "gQGxu8wocWh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Define Machine Learning. What are the main components in Machine Learning?\n",
        "- Machine learning (ML) is a field of artificial intelligence (AI) that enables computers to learn from data, identify patterns, and make predictions or decisions without being explicitly programmed. The main components of machine learning include data, algorithms, models, and predictions.\n",
        "\n",
        "Elaboration :\n",
        "- Data:\n",
        "  - Machine learning models are trained on data, which can be structured or unstructured. This data is used to teach the model how to learn and make predictions.\n",
        "- Algorithms:\n",
        "  - Algorithms are the core of machine learning. They are sets of rules and statistical techniques used to analyze and interpret data. Different algorithms are suited for different tasks, such as classification, regression, clustering, and dimensionality reduction.\n",
        "- Models:\n",
        "  - A model is a representation of the learned relationships and patterns in the data. It's the result of the training process and is used to make predictions on new, unseen data.\n",
        "- Predictions:\n",
        "  - Once a model is trained, it can be used to make predictions or decisions about new data. The accuracy of these predictions depends on the quality of the data, the chosen algorithm, and the training process.\n",
        "- In essence, machine learning empowers computers to learn from experience and improve their performance over time. It's a powerful tool for automating data analysis, making predictions, and building intelligent systems.   "
      ],
      "metadata": {
        "id": "ah6fcXnydYHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.How does loss value help in determining whether the model is good or not?\n",
        "- Loss value serves as a crucial indicator of a model's performance. A lower loss value generally signifies a better-performing model, meaning it's making predictions closer to the actual values. The goal of training a model is to minimize the loss, which represents the difference between the model's predictions and the true labels.\n",
        " - Loss as a Metric :\n",
        "   Loss is a numerical measure of how wrong a model's predictions are. It quantifies the difference between the predicted output and the actual output, according to IBM.\n",
        " - Lower is Better :\n",
        "   A lower loss value indicates that the model is making more accurate predictions, as it's minimizing the error between its predictions and the actual values.\n",
        " - Training and Validation Loss :\n",
        "   Loss is calculated on both training and validation data to assess the model's performance on unseen data.\n",
        " - Understanding Loss Curves :\n",
        "   Plotting the loss value over time (loss curves) can reveal how the model's performance changes during training, says Weights & Biases. A decreasing loss curve suggests the model is learning and improving its performance.\n",
        " - Overfitting and Underfitting :\n",
        "   The loss values can also help identify issues like overfitting (where the model performs well on training data but poorly on new data) or underfitting (where the model is not complex enough to capture the patterns in the data)"
      ],
      "metadata": {
        "id": "ZSp820tCeJwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What are continuous and categorical variables?\n",
        "- In machine learning, continuous variables are numeric and can take any value within a range (like height, weight, or temperature), while categorical variables represent distinct groups or categories (like gender, color, or product type).\n",
        "\n",
        "Elaboration:\n",
        "\n",
        "- Continuous Variables:\n",
        "  - These variables are typically measured and can have infinitely many values between two given values. Examples include:\n",
        "  - Height (a person can be 1.75 meters tall, or 1.751 meters, etc.)\n",
        "  - Weight (a product can weigh 2.3 kilograms, or 2.35 kilograms, etc.)\n",
        "  - Temperature (the temperature can be 25.5 degrees Celsius, or 25.55 degrees Celsius, etc.)\n",
        "- Categorical Variables:\n",
        "  - These variables represent different categories or groups. They can be:\n",
        "  - Nominal: Categories have no inherent order (e.g., colors: red, blue, green).\n",
        "  - Ordinal: Categories have a meaningful order (e.g., education levels: high school, college, graduate school).\n",
        "  - Examples:\n",
        "    - Gender (male/female)\n",
        "    - Product type (laptop, smartphone, tablet)\n",
        "    - City (London, Paris, New York)"
      ],
      "metadata": {
        "id": "YBsQ12WVf04T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "- In machine learning, categorical variables need to be converted into a numerical format suitable for analysis and modeling. Common techniques include one-hot encoding, label encoding, ordinal encoding, and target encoding. One-hot encoding creates a binary column for each category, while label encoding assigns unique integer values. Ordinal encoding preserves the order of categories, and target encoding replaces categories with the average target value for that category.\n",
        "- The common techniques are : -\n",
        "  - One-Hot Encoding\n",
        "  - Label Encoding\n",
        "  - Ordinal Encoding\n",
        "  - Target Encoding\n",
        "  - Binary Encoding\n",
        "  - Frequency Encoding\n",
        "  - Hash Encoding\n",
        "- The choice of technique depends on the nature of the categorical variable (ordinal or nominal), the characteristics of the data, and the requirements of the machine learning model.   "
      ],
      "metadata": {
        "id": "NJ4FR7Qcg6Fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.What do you mean by training and testing a dataset?\n",
        "- In machine learning, a dataset is typically split into training and testing sets. The training set is used to teach the model, while the testing set is used to evaluate its performance after training. This process helps determine how well the model generalizes to new, unseen data.\n",
        "\n",
        "- Here's a more detailed explanation :\n",
        " - Training Data:\n",
        "   - This is the data the model learns from. The model adjusts its internal parameters to minimize errors on this data. The goal is for the model to learn the underlying patterns and relationships within the training data so it can make accurate predictions.\n",
        " - Testing Data:\n",
        "   - Once the model is trained, the testing data is used to assess its performance. This is data the model has not seen during training, so it provides an unbiased measure of how well the model can generalize to new situations. This helps ensure that the model is not just memorizing the training data but has actually learned the underlying patterns.\n",
        "- Importance :\n",
        "  Splitting the data into training and testing sets is crucial for building accurate and reliable machine learning models. It prevents overfitting, where the model learns the training data too well and performs poorly on new data. By using a separate test set, we can evaluate the model's ability to generalize to unseen data and ensure that it's not simply memorizing the training examples.   "
      ],
      "metadata": {
        "id": "_fh7MRqsiPQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is sklearn.preprocessing?\n",
        "- In the context of machine learning, sklearn.preprocessing in scikit-learn (sklearn) is a module that provides a suite of utility functions and transformer classes for transforming raw feature vectors into a representation suitable for downstream estimators.\n",
        "- These preprocessing techniques are crucial for preparing data for machine learning models by cleaning, scaling, normalizing, and encoding it into a format that can be effectively used by the algorithms."
      ],
      "metadata": {
        "id": "OrwjMDKdjEMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame\n",
        "data = {'feature1': [1, 2, 3, 4, 5],\n",
        "        'feature2': ['A', 'B', 'A', 'C', 'B'],\n",
        "        'target': [0, 1, 0, 1, 0]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "df[['feature1']] = scaler.fit_transform(df[['feature1']])\n",
        "\n",
        "# Encode categorical features\n",
        "# The 'sparse' argument is deprecated, so we'll set 'sparse_output' to False instead.\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "encoded_data = encoder.fit_transform(df[['feature2']])\n",
        "df_encoded = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['feature2']))\n",
        "\n",
        "# Concatenate the processed data\n",
        "df_processed = pd.concat([df, df_encoded], axis=1)\n",
        "\n",
        "print(df_processed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsfHCZQgkSbE",
        "outputId": "12250f91-06aa-4d5e-cd72-5d3f3d7efd99"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   feature1 feature2  target  feature2_A  feature2_B  feature2_C\n",
            "0 -1.414214        A       0         1.0         0.0         0.0\n",
            "1 -0.707107        B       1         0.0         1.0         0.0\n",
            "2  0.000000        A       0         1.0         0.0         0.0\n",
            "3  0.707107        C       1         0.0         0.0         1.0\n",
            "4  1.414214        B       0         0.0         1.0         0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What is a Test set?\n",
        "- A test set is a separate portion of the dataset that is held back and not used during the training or model development process. It's used to assess the model's performance on unseen data and provide an unbiased estimate of how well the model will generalize to new, real-world situations.\n",
        "- Using a test set helps prevent overfitting, which is when a model learns the training data too well and performs poorly on new data. It also helps create a more realistic expectation of how the model will perform in a production environment."
      ],
      "metadata": {
        "id": "ocgTWpg9k1gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "- In Python, you can split data for training and testing using the train_test_split function from the scikit-learn library. A typical approach to a machine learning problem involves data collection, preparation, choosing a model, training, evaluating, and making predictions.\n",
        "- Data Splitting in Python :\n",
        "  - Import Libraries:\n",
        "        import pandas as pd\n",
        "        from sklearn.model_selection import train_test_split\n",
        "  - Load Data:\n",
        "        # Assuming your data is in a DataFrame called 'data'\n",
        "        # data = pd.read_csv('your_data.csv')  \n",
        "  - Separate Features (X) and Target (y):\n",
        "        X = data.drop('target_column', axis=1)  # 'target_column' is the column you're predicting\n",
        "        y = data['target_column']\n",
        "  - Split Data:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% training, 20% testing                             \n",
        "    - test_size: Specifies the proportion of the data to allocate to the test set. 0.2 means 20%.\n",
        "    - random_state: Ensures reproducible splitting by setting a seed for the random number generator.        \n",
        "- Machine Learning Problem Approach :\n",
        " - Data Collection: Gather relevant data for your problem.\n",
        " - Data Preparation: Clean, transform, and preprocess the data to make it suitable for training.\n",
        " - Choose a Model: Select an appropriate machine learning algorithm for your problem.\n",
        " - Train the Model: Use the training data (X_train, y_train) to train your chosen model.\n",
        " - Evaluate the Model: Use the test data (X_test, y_test) to evaluate the model's performance.\n",
        " - Parameter Tuning: Adjust the model's hyperparameters to optimize performance.\n",
        " - Make Predictions: Use the trained model to make predictions on new, unseen data.     "
      ],
      "metadata": {
        "id": "ElBO1B6tlPSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.Why do we have to perform EDA before fitting a model to the data?\n",
        "- Exploratory Data Analysis (EDA) is crucial before fitting a model because it helps ensure the data is clean, understandable, and free of errors, ultimately leading to more robust and accurate models.\n",
        "- By exploring the data, we can identify potential issues like missing values, outliers, and inconsistencies, and make informed decisions about data preparation & feature engineering.\n",
        "- This proactive approach minimizes the risk of building models on flawed data, which can lead to poor performance and misleading results.\n",
        "- Why we have to perform EDA : -\n",
        "  - Data quality\n",
        "  - Understanding data structure\n",
        "  - Feature selection\n",
        "  - Model assumption validation\n",
        "  - Improved model performance\n",
        "- In essence, EDA acts as a crucial bridge between raw data and the final model, ensuring that the model is built on a solid foundation of clean, well-understood, and relevant data.   "
      ],
      "metadata": {
        "id": "tpdLnxyenfR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.What is correlation?\n",
        "- In machine learning, correlation refers to the statistical relationship between two or more variables, indicating how much they tend to change together. It's a way to quantify the strength and direction of this relationship, but it doesn't necessarily imply a cause-and-effect relationship."
      ],
      "metadata": {
        "id": "ZotebTQEpIVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What does negative correlation mean?\n",
        "- A negative correlation is a relationship between two variables that move in opposite directions. In other words, when variable A increases, variable B decreases. A negative correlation is also known as an inverse correlation."
      ],
      "metadata": {
        "id": "DXk9oswappNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.How can you find correlation between variables in Python?\n",
        "- To find the correlation between variables in Python, various methods and libraries can be employed. The most common approaches involve using the pandas library for data manipulation and the NumPy or SciPy libraries for numerical computations.\n",
        "- Using pandas\n",
        "  - The pandas library provides the corr() method for DataFrames, which calculates the correlation matrix between variables. By default, it computes the Pearson correlation coefficient, but other methods like Spearman and Kendall can also be specified.\n",
        "- Using SciPy\n",
        "  - The SciPy library offers functions to calculate different correlation coefficients, such as Pearson, Spearman, and Kendall. These functions are useful when working with arrays or when specific correlation methods are needed.\n",
        "- Visualizing Correlations\n",
        "  - Correlation matrices can be visualized using heatmaps, which provide an intuitive way to understand the relationships between variables. The seaborn library simplifies the creation of heatmaps from correlation matrices.    \n",
        "- These methods allow for a comprehensive analysis of the relationships between variables in a dataset, aiding in feature selection, model building, and data interpretation.  "
      ],
      "metadata": {
        "id": "C6qtDdP5p1-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame\n",
        "data = {'A': [1, 2, 3, 4, 5],\n",
        "        'B': [2, 4, 5, 4, 5],\n",
        "        'C': [5, 4, 3, 2, 1]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "print(corr_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WWzKBSRqoz9",
        "outputId": "c43981b7-6a9a-4d2a-ae85-d6735e4689e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          A         B         C\n",
            "A  1.000000  0.774597 -1.000000\n",
            "B  0.774597  1.000000 -0.774597\n",
            "C -1.000000 -0.774597  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "z = np.array([5, 4, 3, 2, 1])\n",
        "\n",
        "# Calculate Pearson correlation\n",
        "pearson_corr, _ = pearsonr(x, y)\n",
        "print(f\"Pearson correlation between x and y: {pearson_corr}\")\n",
        "\n",
        "# Calculate Spearman correlation\n",
        "spearman_corr, _ = spearmanr(x, z)\n",
        "print(f\"Spearman correlation between x and z: {spearman_corr}\")\n",
        "\n",
        "# Calculate Kendall correlation\n",
        "kendall_corr, _ = kendalltau(x, z)\n",
        "print(f\"Kendall correlation between x and z: {kendall_corr}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12OT9Tw-qyqW",
        "outputId": "374f1b98-ed28-4b54-8939-b6812665299e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson correlation between x and y: 0.7745966692414834\n",
            "Spearman correlation between x and z: -0.9999999999999999\n",
            "Kendall correlation between x and z: -0.9999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample DataFrame\n",
        "data = {'A': [1, 2, 3, 4, 5],\n",
        "        'B': [2, 4, 5, 4, 5],\n",
        "        'C': [5, 4, 3, 2, 1]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Create heatmap\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "POnuV_zaq2Rd",
        "outputId": "a3ceaa2c-cb34-4132-87f3-98c9b9734e07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.What is causation? Explain difference between correlation and causation with an example.\n",
        "- Causation means one event directly leads to another, while correlation indicates a relationship between two events, but doesn't necessarily mean one caused the other. For example, ice cream sales and pool drownings are correlated because they both increase during summer, but eating ice cream doesn't cause drownings.\n",
        "- Key Differences :\n",
        "  - Directionality:\n",
        "    - Causation implies a direction, with one variable influencing the other. Correlation does not.\n",
        "  - Strength of Relationship:\n",
        "    - Causation establishes a direct link, while correlation only shows an association.\n",
        "  - Manipulation:\n",
        "    - In experimental studies, researchers can manipulate an independent variable to see its effect on a dependent variable, establishing a causal link. In correlational studies, researchers only observe relationships without manipulating variables.\n",
        "- Example:\n",
        "  There's a positive correlation between ice cream sales and pool drownings. Both increase during summer, but one doesn't cause the other. The hot weather and people's desire to be outside and enjoy it are the underlying causes of both events.     "
      ],
      "metadata": {
        "id": "iptJKrNXrT6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "- In machine learning, an optimizer is an algorithm that adjusts a model's parameters (like weights and biases) to minimize a loss function, making the model more accurate during training. It iteratively updates these parameters based on the direction and magnitude of the loss function's gradient.\n",
        "- Here are some common types of optimizers :\n",
        " - Gradient Descent (GD)\n",
        "   - Explanation:\n",
        "    - A fundamental optimization algorithm that iteratively updates parameters in the direction of the steepest descent of the loss function. It uses the entire dataset to calculate the gradient, making it computationally expensive for large datasets but often converges to a stable solution.\n",
        "   - Example:\n",
        "    - Imagine a hiker trying to descend a mountain. Gradient descent is like the hiker taking a step in the direction of the steepest downward slope to reach the bottom (the minimum loss).\n",
        " - Stochastic Gradient Descent (SGD)\n",
        "   - Explanation:\n",
        "    - An improved version of GD where parameters are updated using the gradient calculated on a single random sample from the dataset at each iteration. This makes it much faster than GD, especially for large datasets, but might result in noisier updates and potentially oscillate around the minimum.\n",
        "   - Example:\n",
        "    - The hiker now takes a step based on the slope at their current location, which can vary depending on the terrain. This can be faster but less stable than using the average slope of the entire mountain.  \n",
        " - Mini-batch Gradient Descent\n",
        "   - Explanation:\n",
        "    - A compromise between GD and SGD, where parameters are updated using the gradient calculated on a small random subset (a mini-batch) of the dataset. This approach offers a balance between computational efficiency and stability, allowing for faster convergence than GD while reducing noise compared to SGD.\n",
        "   - Example:\n",
        "    - The hiker now takes a step based on the average slope within a small local area. This offers a more stable and efficient descent than taking steps based on just one spot.     \n",
        " - Momentum-based Optimizers\n",
        "   - Explanation:\n",
        "    - These optimizers (like SGD with momentum, Nesterov Accelerated Gradient - NAG) use a \"momentum\" term that accumulates past gradients to accelerate the optimization process, especially in directions where gradients are consistent. This helps the model escape local minima and converge faster.\n",
        "   - Example:\n",
        "    - Imagine the hiker now has a \"momentum\" like a rolling ball. The ball will tend to roll in the direction of the accumulated downward force (gradients), even if the current slope is slightly uphill.     \n",
        " - Adaptive Learning Rate Optimizers\n",
        "   - Explanation:\n",
        "    - These optimizers (like Adagrad, RMSprop, Adam) dynamically adjust the learning rate for each parameter based on its historical gradients. This can be particularly useful when dealing with datasets that have varying scales and features, as it allows the optimizer to adapt to the specific characteristics of each parameter.\n",
        "   - Example:\n",
        "    - The hiker now has a clever pair of shoes. The shoes adjust their grip depending on the type of terrain. For steep slopes, they grip tightly, and for flat areas, they grip less to avoid getting stuck.    \n",
        "- In summary: Optimizers are crucial tools in machine learning for training models. Different optimizers offer different trade-offs between computational efficiency, convergence speed, and stability, and the best choice depends on the specific dataset and model.     "
      ],
      "metadata": {
        "id": "XfL-ZYzQt3XC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.What is sklearn.linear_model ?\n",
        "- The sklearn.linear_model module in scikit-learn is a collection of classes and functions for performing machine learning tasks using linear models. Linear models predict a target variable by calculating a weighted sum of input features. The module includes various algorithms for regression and classification, each suited for different data characteristics and problem types.\n",
        "- Key Linear Models in sklearn.linear_model"
      ],
      "metadata": {
        "id": "iRQ0Wv-_vg6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression: A fundamental model that predicts a continuous target variable by finding the best linear relationship between the features and the target. It minimizes the sum of squared differences between the observed and predicted values."
      ],
      "metadata": {
        "id": "HKeGG_2Bwycs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()"
      ],
      "metadata": {
        "id": "9XXv5rfiwOxX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge Regression: An extension of linear regression that adds a regularization term to prevent overfitting, particularly useful when dealing with multicollinearity (high correlation between features)."
      ],
      "metadata": {
        "id": "sEdtIIsCwiqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "model = Ridge(alpha=1.0)"
      ],
      "metadata": {
        "id": "7zLuBqXww6yD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso Regression: Another regularized linear regression model that uses L1 regularization, which can lead to sparse solutions where some feature weights are zero, effectively performing feature selection."
      ],
      "metadata": {
        "id": "M4mMbkfYxD8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "model = Lasso(alpha=0.1)"
      ],
      "metadata": {
        "id": "ZSvF_O3hxIk0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression: A classification algorithm that predicts the probability of a binary outcome. It uses a sigmoid function to map the linear combination of features to a probability between 0 and 1."
      ],
      "metadata": {
        "id": "8BC2JZ7xxQmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()"
      ],
      "metadata": {
        "id": "IY9FGBKVxUrb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGDClassifier and SGDRegressor: Implementations of linear models (for classification and regression, respectively) that use stochastic gradient descent for training, making them suitable for large datasets."
      ],
      "metadata": {
        "id": "hUbGydqwxahC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
        "classifier = SGDClassifier()\n",
        "regressor = SGDRegressor()"
      ],
      "metadata": {
        "id": "s0WJo3IfxemR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.What does model.fit() do? What arguments must be given?\n",
        "- In the context of machine learning, model.fit() trains the model using the provided data. It's the core function where the model learns the relationships between input features and target variables. The primary arguments to fit() are the training data (x) and the corresponding target values (y).\n",
        "- Arguments:\n",
        "  - x: Represents the input data, which can be a NumPy array, a list of arrays, or a dictionary mapping input names to arrays, depending on the model and its inputs.\n",
        "  - y: Represents the target values, which are the known outputs for the input data. It follows the same structure as x but for the model's outputs.\n",
        "- Other Arguments:\n",
        "  - batch_size: Controls how many samples are processed at a time during training.\n",
        "  - epochs: Determines how many times the entire dataset is passed through the model during training.\n",
        "  - validation_data: Provides a separate dataset for evaluating the model's performance during training (optional).\n",
        "  - class_weight: Allows assigning different weights to different classes for imbalanced datasets (optional).\n",
        "  - sample_weight: Allows assigning weights to individual samples in the training data (optional).   "
      ],
      "metadata": {
        "id": "ovN8yi6xxl2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.What does model.predict() do? What arguments must be given?\n",
        "- The model.predict() function in machine learning, particularly within libraries like Keras or scikit-learn, is used to make predictions on new, unseen data using a trained model. It passes the input data through the model and returns the output, which could be a prediction, probability, or other relevant value depending on the model's task.\n",
        "- Arguments:\n",
        "  - Input data:\n",
        "    - The primary argument is the new data you want to make predictions on. This is often represented as a NumPy array or a similar data structure depending on the library.\n",
        "- Other optional arguments:\n",
        "  - Depending on the library and model, there might be additional arguments like batch_size (to control how many samples are processed at once), verbose (to control the level of output during the prediction process), or use_parallel (to enable parallel processing).\n",
        "- Example (Keras):  "
      ],
      "metadata": {
        "id": "AFMATk7RyID5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "test_input = np.array([[1, 2, 3, 4]])\n",
        "\n",
        "model = LogisticRegression()"
      ],
      "metadata": {
        "id": "1JYZjHP0y_b-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In this Keras example, the model.predict() function takes the test_input array and passes it through the trained model. The predictions variable will then contain the model's output for that input data."
      ],
      "metadata": {
        "id": "KD8AV-Doz_hp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.What are continuous and categorical variables?\n",
        "- In machine learning, variables are classified as continuous or categorical based on the type of data they represent. Continuous variables can take on any value within a range (like height or temperature), while categorical variables represent distinct groups or categories (like gender or colors).\n",
        "- Continuous Variables :\n",
        "  - Definition:\n",
        "    - These variables can take on any value within a specific range. They are often measured and can have infinitely many values between any two given values.\n",
        "  - Examples:\n",
        "    - Height, weight, age, temperature, salary.\n",
        "  - Characteristics:\n",
        "    - They are typically numerical and can be subject to mathematical operations.\n",
        "  - Note:\n",
        "    - Continuous variables can sometimes be treated as categorical by grouping them into intervals or categories, but this can result in information loss.\n",
        "- Categorical Variables:\n",
        "  - Definition:\n",
        "    - These variables represent data that falls into distinct categories or groups.\n",
        "  - Examples:\n",
        "    - Gender (male/female), color (red/blue/green), type of car (sedan/SUV/truck).\n",
        "  - Characteristics:\n",
        "    - They are not numerical and are often described using labels or names.\n",
        "  - Subtypes:\n",
        "    - Categorical variables can be further divided into:\n",
        "      - Nominal: Categories have no inherent order or ranking (e.g., colors).\n",
        "      - Ordinal: Categories have a meaningful order or ranking (e.g., education level - high school, college, graduate).      "
      ],
      "metadata": {
        "id": "bgLMcIlV0BkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.What is feature scaling? How does it help in Machine Learning?\n",
        "- Feature scaling in machine learning is a pre-processing technique that transforms numerical features to a common scale, ensuring that all features contribute equally to the model's training process. This helps prevent features with larger ranges or magnitudes from dominating the model, leading to improved accuracy and efficiency, especially for algorithms that rely on distance or gradient calculations.\n",
        "- How does it helps in machine learning : -\n",
        "  - Prevents dominance of larger features:\n",
        "    - Without scaling, a feature with a wider range of values might be given more weight by the model, even if it's not the most important feature.\n",
        "  - Improves performance of distance-based algorithms:\n",
        "    - Algorithms like k-Nearest Neighbors (k-NN) and K-Means Clustering are sensitive to the scale of features. Scaling ensures that distances between data points are calculated more fairly, according to Applied AI Course.\n",
        "  - Faster convergence of optimization algorithms:\n",
        "    - Many optimization algorithms, like gradient descent, converge faster when features are on a similar scale, notes a post on Medium.\n",
        "  - More accurate model predictions:\n",
        "    - By preventing certain features from dominating, scaling helps the model to learn more accurate and robust relationships between features and target variables, according to a post on Medium."
      ],
      "metadata": {
        "id": "ENAwZ1ef1EvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.How do we perform scaling in Python?\n",
        "- Data scaling in Python involves transforming numerical data to a standard range, which is crucial for many machine learning algorithms. Several methods can be used for scaling, with the most common ones being normalization and standardization.\n",
        "-  Min-Max Scaling (Normalization)\n",
        "This method scales data to a range between 0 and 1."
      ],
      "metadata": {
        "id": "NCKea7pi13ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[1, 5], [2, 8], [3, 6]])\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjwAU0zh2IdN",
        "outputId": "97df0e85-a3b9-4a2a-ef1c-53b795531c4a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.        ]\n",
            " [0.5        1.        ]\n",
            " [1.         0.33333333]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Standardization (Z-score normalization)\n",
        "This method scales data to have a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "EQhrP-d72MsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[1, 5], [2, 8], [3, 6]])\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKsSOg9a2SLI",
        "outputId": "c39faa04-bf38-4686-9128-53bae52cd7f6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.06904497]\n",
            " [ 0.          1.33630621]\n",
            " [ 1.22474487 -0.26726124]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Robust Scaling\n",
        "This method is similar to standardization but uses the median and interquartile range, making it more robust to outliers."
      ],
      "metadata": {
        "id": "jpplICZX2YsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[1, 5], [2, 8], [3, 6]])\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBqdW3BB2c5_",
        "outputId": "11bd523f-5ada-41aa-dc61-4e47cb6f66e0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.         -0.66666667]\n",
            " [ 0.          1.33333333]\n",
            " [ 1.          0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- MaxAbs Scaling\n",
        "This method scales data to the range [-1, 1] by dividing by the maximum absolute value."
      ],
      "metadata": {
        "id": "UeicBmHf2gk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[1, -5], [-2, 8], [3, -6]])\n",
        "scaler = MaxAbsScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gSDC3GD2kre",
        "outputId": "21ac9973-e55f-46a9-eca5-bdb021eac269"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.33333333 -0.625     ]\n",
            " [-0.66666667  1.        ]\n",
            " [ 1.         -0.75      ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- These scaling techniques can be implemented using the scikit-learn library, which provides classes for each method. It's important to choose the appropriate scaling method based on the data distribution and the requirements of the machine learning algorithm being used."
      ],
      "metadata": {
        "id": "Bp8TBveJ2o17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.What is sklearn.preprocessing?\n",
        "- The sklearn.preprocessing module in scikit-learn provides functions and classes to transform raw data into a suitable format for machine learning algorithms.\n",
        "- Data preprocessing is a crucial step in the machine learning workflow because it helps to improve the accuracy and efficiency of the models.\n",
        "- It involves cleaning, transforming, and scaling data to ensure that it is consistent and in a format that the algorithms can understand."
      ],
      "metadata": {
        "id": "kAx0XaXG2tAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.How do we split data for model fitting (training and testing) in Python?\n",
        "- To split data for model fitting, training, and testing in Python, the most common approach is to use scikit-learn's `train_test_split` function.\n",
        "- This function randomly divides your dataset into training and testing subsets. You can also split data into training, validation, and test sets using this function in two steps."
      ],
      "metadata": {
        "id": "MF1Dm1b22_xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Explain data encoding?\n",
        "- Data encoding is the process of converting data from one format to another, typically to make it easier to store, transmit, or process. This involves transforming data into a specific code or representation that a computer or system can understand and utilize. Encoding is often used for security, compression, and compatibility between different systems.\n",
        "-Examples of encoding :\n",
        "  - Text encoding:\n",
        "    - Converting text into a format like ASCII or Unicode, which uses specific numerical codes to represent characters, allowing computers to understand and process different languages and symbols.\n",
        "  - Image encoding:\n",
        "    - Compressing images using formats like JPEG or PNG, which use various encoding techniques to reduce file size without losing too much quality.\n",
        "  - Audio encoding:\n",
        "    - Converting audio into formats like MP3 or AAC, which use encoding to reduce file size and make it easier to store and transmit.\n",
        "  - Data compression:\n",
        "    - Using algorithms like LZW or Huffman coding to reduce the size of data files by identifying and replacing repetitive patterns with shorter codes.\n",
        "  - Encryption:\n",
        "    - Using algorithms like AES or RSA to encrypt data, making it unreadable to unauthorized users.\n",
        "- In essence, data encoding is a fundamental process in computer science and technology that enables efficient and secure data management and communication.     "
      ],
      "metadata": {
        "id": "rAaZGZG337Bi"
      }
    }
  ]
}